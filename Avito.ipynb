{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "/home/dsxuser/.kaggle/competitions/avito-demand-prediction/train_active.csv.zip\n07:06 [Reading train_active.csv.zip data>] done in 3 m,7 s\n==========================================================================================\n07:06 [Processing train_active.csv.zip column glove_data/description] done in 0 m,0 s\n==========================================================================================\nchunks total 4\n"
                }
            ], 
            "source": "import time\nimport gc\nimport re\nimport csv\nimport string\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport matplotlib.pyplot as plt\nfrom scipy.sparse import hstack\n\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom contextlib import contextmanager\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom nltk.corpus import stopwords\nimport warnings\n\nwarnings.filterwarnings('ignore')\nrussian_stop = set(stopwords.words('russian'))\n\n\nPath = \"/home/dsxuser/.kaggle/competitions/avito-demand-prediction/\"\n# Path = \"../input/\"\n\ntarget = 'deal_probability'\nindex = 'item_id'\nFNAS = ''\nV_SPLT = .2\nencode = True\nwith open('/home/dsxuser/.kaggle/datasets/sophieg/wikipedia-language-iso639/unicode_vars.pkl', 'rb') as f:\n    EMOJI, C_ALPHA = pickle.load(f)\nEMOJI = \"\".join(EMOJI)\nC_ALPHA = \"\".join(C_ALPHA)\nPUNC = re.escape(string.punctuation)\nNUM = string.digits\nA_ALPHA = string.ascii_letters\nWS = ' '\ntempl = '(?P<la>[{0}])(?P<not>[^ ])'\ntempr = '(?P<not>[^ ])(?P<ra>[{0}])'\nlrepl = r'\\g<la> \\g<not>'\nrrepl = r'\\g<not> \\g<ra>'\nlalpha = r'(?P<la>(?:[{0}{1}]+))(?P<not>[^ {0}{1}])'.format(A_ALPHA, C_ALPHA)\nralpha = r'(?P<not>[^ {0}{1}])(?P<ra>(?:[{0}{1}]+))'.format(A_ALPHA, C_ALPHA)\nnot_valid_char = u'[^{}{}{}{}{}{}]'.format(\n    A_ALPHA, C_ALPHA, NUM, PUNC, EMOJI, WS)\nrepl_dict = {\n    # if a Punctuation sequence is not followed by whitespace\n    # add it\n    # punct on the left\n    templ.format(PUNC): lrepl,\n    # if a Punctuation sequence is not preceeded by whitespace\n    # add it\n    # punct on the right\n    tempr.format(PUNC): rrepl,\n    # if a number sequence is not followed by whitespace\n    # add it\n    # nums on the left\n    templ.format(NUM): lrepl,\n    # if a number sequence is not preceeded by whitespace\n    # add it\n    # nums on the right\n    tempr.format(NUM): rrepl,\n    # if a number sequence is not followed by whitespace\n    # add it\n    # emoji on the left\n    templ.format(EMOJI): lrepl,\n    # if an emoji sequence is not preceeded by whitespace\n    # add it\n    # emoji on the right\n    tempr.format(EMOJI): rrepl,\n    # any sequence of alpha chars should end with a white space\n    # alpha on the left\n    lalpha: lrepl,\n    # alpha on the right\n    ralpha: rrepl,\n    # remove extra whitespace\n    r'\\s+': ' '\n}\ntext_cols = ['description', 'title',\n             'param_1', 'param_2', 'param_3']\n\nfiles = ['train_active.csv.zip',\n         'test_active.csv.zip',\n         'train.csv.zip',\n         'test.csv.zip']\n\n\n@contextmanager\ndef timer(name, verbose=True):\n    t0 = time.time()\n    yield\n    if verbose:\n        t1 = time.time()\n        m = int((t1 - t0) // 60)\n        s = int((t1 - t0) % 60)\n        tm = time.strftime(\"%H:%M\")\n        print('{} [{}] done in {} m,{} s\\n{}'.format(tm,name,m, s,\"=\"*90))\n\n\ndata = {}\ndtypes = {**{index: str}, **dict(zip(text_cols, [str] * len(text_cols)))}\nfor j, fn in enumerate(files):\n    with timer('Reading {} data>'.format(fn)):\n        f = '{}{}'.format(Path, fn)\n        print(f)\n        if fn in ['train.csv.zip', 'test.csv.zip']:\n            data[fn] = pd.read_csv(f, usecols=[index, *text_cols],\n                                   dtype=dtypes,\n                                   delimiter=',',\n                                   quotechar='\"',\n                                   quoting=csv.QUOTE_MINIMAL,\n                                   na_filter=False,\n                                   index_col=index)\n        else:\n            data[fn] = pd.read_csv(f, usecols=text_cols,\n                                   dtype=dtypes,\n                                   delimiter=',',\n                                   quotechar='\"',\n                                   quoting=csv.QUOTE_MINIMAL,\n                                   na_filter=False)\n    for c in text_cols:\n        with timer('Processing {} column glove_data/{}'.format(fn, c)):\n            pass\n        chunk_size = 4000000\n        iters = int(np.ceil(data[fn].shape[0] / chunk_size))\n        print('chunks total', iters)\n        for i in range(iters):\n            with timer('processing column {} {} chunk - {} / {}'.format(c,fn,i+1,iters)):\n                start = i * chunk_size\n                end = (i + 1) * chunk_size\n                res = data[fn][c].iloc[start: end].str.split(\n                    not_valid_char).str.join(' ').replace(\n                    repl_dict,\n                    regex=True)\n            with timer('Writing column {} {} chunk - {} / {}'.format(c,fn,i+1,iters)):\n                with open('pre_tfidf/{}_agg.txt'.format(c), mode='a',\n                          newline='\\n') as f:\n                    f.write('   '.join(res.values))\n\n                if fn in ['train.csv.zip', 'test.csv.zip']:\n                    res.to_csv('pre_tfidf/{}_{}'.format(c,fn), index=True,\n                               header=True, mode='a')\n                del res\n                gc.collect()\n    del data[fn]\n    gc.collect()\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!mkdir preprocessing"
        }, 
        {
            "execution_count": 55, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Requirement not upgraded as not directly required: kaggle in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\nRequirement not upgraded as not directly required: urllib3>=1.15 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from kaggle)\nRequirement not upgraded as not directly required: six>=1.10 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from kaggle)\nRequirement not upgraded as not directly required: python-dateutil in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from kaggle)\nRequirement not upgraded as not directly required: requests in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from kaggle)\nRequirement not upgraded as not directly required: certifi in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from kaggle)\nRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests->kaggle)\nRequirement not upgraded as not directly required: idna<2.7,>=2.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests->kaggle)\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\ntrain.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\ntest.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\ntrain_active.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\ntest_active.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\nperiods_train.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\nperiods_test.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\nsample_submission.csv: Skipping, found more recently modified local copy (use --force to force download)\n--2018-05-26 06:57:28--  https://www.kaggle.com/sophieg/wikipedia-language-iso639/downloads/unicode_vars.pkl/2\nResolving www.kaggle.com (www.kaggle.com)... 168.62.224.124\nConnecting to www.kaggle.com (www.kaggle.com)|168.62.224.124|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: /account/login?returnUrl=%2Fsophieg%2Fwikipedia-language-iso639%2Fversion%2F2 [following]\n--2018-05-26 06:57:28--  https://www.kaggle.com/account/login?returnUrl=%2Fsophieg%2Fwikipedia-language-iso639%2Fversion%2F2\nReusing existing connection to www.kaggle.com:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 8229 (8.0K) [text/html]\nSaving to: \u20182\u2019\n\n100%[======================================>] 8,229       --.-K/s   in 0s      \n\n2018-05-26 06:57:28 (89.6 MB/s) - \u20182\u2019 saved [8229/8229]\n\n"
                }
            ], 
            "source": "# setup\n# !pip install kaggle\n# s ='{\"username\":\"sophieg\",\"key\":\"10bad49b19d9ae80a9c0d7369d58f140\"}'\n# open('/home/dsxuser/.kaggle/kaggle.json','w').write(s)\n# !chmod 600 /home/dsxuser/.kaggle/kaggle.json\n# import nltk\n# nltk.download('stopwords')\n# !kaggle competitions download -c avito-demand-prediction -f train.csv.zip\n# !kaggle competitions download -c avito-demand-prediction -f test.csv.zip\n# !kaggle competitions download -c avito-demand-prediction -f train_active.csv.zip\n# !kaggle competitions download -c avito-demand-prediction -f test_active.csv.zip\n# !kaggle competitions download -c avito-demand-prediction -f periods_train.csv.zip\n# !kaggle competitions download -c avito-demand-prediction -f periods_test.csv.zip\n# !kaggle competitions download -c avito-demand-prediction -f sample_submission.csv\n# !kaggle datasets download -d sophieg/wikipedia-language-iso639 -f unicode_vars.pkl\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# # ***********************text start****************************\n\n\ncols = ['description', 'title',\n        'param_1', 'param_2', 'param_3']\nfor i, c in enumerate(cols):\n    print(c)\n    fn = f'pre_tfidf/{c}_agg.txt'\n    tk = TfidfVectorizer(tokenizer=lambda x: x.split(),\n                         ngram_range=(1, 2),\n                         norm='l2',\n                         smooth_idf=True,\n                         binary=False,\n                         token_pattern=r'(?u)\\b\\S+\\b',\n                         lowercase=True,\n                         strip_accents=None)\n    # with timer(f'reading agg file {fn} > {c} column'):\n    #     text = pd.read_csv(fn,\n    #                        delimiter=',',\n    #                        quotechar='\"',\n    #                        quoting=csv.QUOTE_MINIMAL,\n    #                        na_filter=False,\n    #                        squeeze=True)\n    with timer(f'fitting tfidf vectoriser> {c} column'):\n        with open(fn, 'r') as f:\n            tk.fit(f)\n    with timer(f'Transforming train via tfidf vectoriser> {c} column'):\n        train = pd.read_csv(f'pre_tfidf/{c}_train.csv', index_col=0,\n                            delimiter=',',\n                            quotechar='\"',\n                            quoting=csv.QUOTE_MINIMAL,\n                            na_filter=False)\n        x = tk.transform(train).astype(np.float32)\n        del train\n        gc.collect()\n    with timer(f'adding up train word vector> {c} column'):\n        css = ['{}_{}'.format(c, w.replace(\n            '<', 'less_than').replace(\n            '[', 'open_bracket').replace(\n            ']', 'close_bracket'))for w in tk.get_feature_names()]\n        if i == 0:\n            Xt = x\n            txt_cols = css\n            idf = tk.idf_\n        else:\n            Xt = hstack([Xt, x])\n            txt_cols = [*txt_cols, *css]\n            idf = hstack([idf, tk.idf_])\n        txt_types = dict(zip(txt_cols, ['float'] * len(txt_cols)))\n        del x\n        gc.collect()\n    with timer(f'Transforming test via tfidf  vectoriser> {c} column'):\n        test = pd.read_csv(f'pre_tfidf/{c}_test.csv', index_col=0,\n                           delimiter=',',\n                           quotechar='\"',\n                           quoting=csv.QUOTE_MINIMAL,\n                           na_filter=False)\n        x = tk.transform(test).astype(np.float32)\n        del test\n        gc.collect()\n    with timer(f'adding up test word vector> {c} column'):\n        if i == 0:\n            Xte = x\n        else:\n            Xte = hstack([Xte, x])\n        del x\n        gc.collect()\n    del tk\n    gc.collect()\nnp.save('Xt', Xt)\nnp.save('Xte', Xte)\ndel Xt, Xte\ngc.collect()\n# ***********************text ends****************************"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\n# aggregate period info*************************\ncols = ['item_id', 'date_from', 'date_to']\nwith timer('Reading train raw data /aggregated data'):\n    train_periods = pd.read_csv(f'{Path}periods_train.csv',\n                                parse_dates=['date_from', 'date_to'],\n                                engine='c',\n                                usecols=cols)\nwith timer('Reading test raw data/aggregated data'):\n    test_periods = pd.read_csv(f'{Path}periods_test.csv',\n                               parse_dates=['date_from', 'date_to'],\n                               engine='c',\n                               usecols=cols)\nwith timer('Concatenating all data'):\n    p = train_periods.append(test_periods)\ndel train_periods, test_periods\ngc.collect()\nwith timer('Calculating the up time'):\n    conv_fact = 1. / ((10 ** 9) * 60. * 60. * 24.)  # convert msec to days\n    p['duration'] = (p[['date_from', 'date_to']].astype(int).diff(\n        axis=1)['date_to'] * conv_fact).astype(np.int32)\n    p.drop(cols[1:], axis=1, inplace=True)\n    gc.collect()\nwith timer('Aggregating period data'):\n    grs = p.groupby('item_id')\n    periods = grs['duration'].agg(\n        ['sum',\n         'count',\n         'std']).rename(columns={'sum': 'total_up_time',\n                                 'count': 'ad-freq',\n                                 'std': 'spread'}).reset_index()\n#     periods['spread'].fillna(0,inplace=True)\n    p = p.merge(periods, on='item_id', how='left')\ndel periods, grs\ngc.collect()\n\n# aggregate period info end*************************\n# ***************** agg floats ended **************************************\ncats_dict = {'user_id': 'user',\n             'parent_category_name': 'pcat',\n             'category_name': 'cat'}\ncols = ['total_up_time', 'ad-freq', 'spread']\nfor i, (g_col, g_col_ab) in enumerate(cats_dict.items()):\n    used_cols = ['item_id', g_col]\n    with timer(f'Reading data/aggregated data {g_col}'):\n        train = pd.read_csv(f'{Path}train.csv',\n                            usecols=used_cols,\n                            engine='c')\n        train_active = pd.read_csv(f'{Path}train_active.csv',\n                                   usecols=used_cols,\n                                   engine='c')\n        test = pd.read_csv(f'{Path}test.csv',\n                           usecols=used_cols,\n                           engine='c')\n        test_active = pd.read_csv(f'{Path}/test_active.csv',\n                                  usecols=used_cols,\n                                  engine='c')\n        d = train.append(test).append(train_active).append(\n            test_active).drop_duplicates(['item_id'])\n        del train_active, test_active, test, train\n        gc.collect()\n    with timer(f'Collating aggregated data {g_col}'):\n        df = p.merge(d, on='item_id', how='left')\n        del d\n        gc.collect()\n        grs = df.groupby(g_col)\n    with timer(f'Collating {g_col} stats'):\n        agg = pd.concat([grs[cols].mean().rename(\n            columns=lambda x: f'avg_{g_col_ab}_{x}'),\n            grs[cols].median().rename(\n            columns=lambda x: f'median_{g_col_ab}_{x}'),\n            grs[cols].max().rename(\n            columns=lambda x: f'max_{g_col_ab}_{x}'),\n            grs[cols].min().rename(\n            columns=lambda x: f'min_{g_col_ab}_{x}'),\n            grs[cols].var().rename(\n            columns=lambda x: f'spread_{g_col_ab}_{x}'),\n            grs['item_id'].count().to_frame().rename(\n            columns=lambda x: f'cnt_{g_col_ab}_{x}')],\n            axis=1).reset_index()\n\n    del grs, df\n    gc.collect()\n    with timer(f'Substituting {g_col} into data'):\n        train = pd.read_csv(f'{Path}train.csv',\n                            usecols=used_cols,\n                            engine='c')\n        r = train.merge(agg, on=g_col, how='left').set_index('item_id')\n        r.drop(g_col, axis=1, inplace=True)\n        if i == 0:\n            X = r\n        else:\n            X = X.join(r)\n        del r, train\n        gc.collect()\n        test = pd.read_csv(f'{Path}test.csv',\n                           usecols=used_cols,\n                           engine='c')\n        r = test.merge(agg, on=g_col, how='left').set_index('item_id')\n        r.drop(g_col, axis=1, inplace=True)\n        r = r[r.columns[r.describe().loc['std'] > 0]]\n        if i == 0:\n            Xe = r\n        else:\n            Xe = Xe.join(r)\n        del test, agg, r\n        gc.collect()\nflt_types = X.dtypes.str.replace(r'^int.*$', 'q').str.replace(\n    r'^float.*$', 'float').todict()\n# ***************** agg floats ended **************************************\n\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\n# # ***********************start floats****************************\ntext_cols = ['title', 'description',\n             'param_1', 'param_2', 'param_3']\n\n\ncols = [index, 'price', *text_cols, target]\n\nwith timer('Reading raw data/ float columns'):\n    train = pd.read_csv(f'{Path}/train.csv',\n                        engine='c',\n                        index_col=index,\n                        usecols=cols)\n    test = pd.read_csv(f'{Path}/test.csv',\n                       engine='c',\n                       index_col=index,\n                       usecols=cols[:-1])\ny = train[target]\ntrain.drop(target, axis=1, inplace=True)\ngc.collect()\nwith timer('starting text stats'):\n    d = train.append(test)\n\nwith timer('total length stat'):\n    df = d[text_cols].applymap(\n        lambda x: len(x) if type(x) == str else x).astype(np.float32).rename(\n        columns=lambda x: f\"len_{x}\")\n\nwith timer('word length stat'):\n    r = d[text_cols].applymap(\n        lambda x: len(x.split())\n        if type(x) == str else x).astype(\n        np.float32).rename(columns=lambda x: f\"word_len_{x}\")\n    df = df.join(r)\nwith timer('unique word length stat'):\n    r = d[text_cols].applymap(\n        lambda x: len(list(set(x.split())))\n        if type(x) == str else x).astype(np.float32).rename(\n        columns=lambda x: f\"unq_wrd_len_{x}\")\n    df = df.join(r)\nwith timer('number of new lines stat'):\n    r = d[text_cols].applymap(\n        lambda x: len(re.findall(r'\\n', x))\n        if type(x) == str else x).astype(\n        np.float32).rename(columns=lambda x: f\"new_lines_{x}\")\n    r = r[r.columns[r.describe().loc['std'] > 0]]\n    df = df.join(r)\nwith timer('number of white spaces stat'):\n    r = d[text_cols].applymap(\n        lambda x: len(re.findall(r'\\s', x))\n        if type(x) == str else x).astype(\n        np.float32).rename(columns=lambda x: f\"white_space_{x}\")\n    r = r[r.columns[r.describe().loc['std'] > 0]]\n    df = df.join(r)\nwith timer('number of caps stat'):\n    r = d[text_cols].applymap(\n        lambda x: len(re.findall(r'[A-Z\u0410-\u042f]', x))\n        if type(x) == str else x).astype(\n        np.float32).rename(columns=lambda x: f\"caps_{x}\")\n    r = r[r.columns[r.describe().loc['std'] > 0]]\n    df = df.join(r)\nwith timer('number of smalls stat'):\n    r = d[text_cols].applymap(\n        lambda x: len(re.findall(r'[a-z\u0430-\u044f]', x))\n        if type(x) == str else x).astype(\n        np.float32).rename(columns=lambda x: f\"smalls_{x}\")\n    r = r[r.columns[r.describe().loc['std'] > 0]]\n    df = df.join(r)\n\nwith timer('number of digits stat'):\n    r = d[text_cols].applymap(\n        lambda x: len(re.findall(r'[0-9]', x))\n        if type(x) == str else x).astype(\n        np.float32).rename(columns=lambda x: f\"dig_{x}\")\n    r = r[r.columns[r.describe().loc['std'] > 0]]\n    df = df.join(r)\nwith timer('number of punctuation stat'):\n    r = d[text_cols].applymap(\n        lambda x: len(re.findall(fr'[{re.escape(string.punctuation)}]',\n                                 x)) if type(x) == str else x).astype(\n        np.float32).rename(columns=lambda x: f\"punct_{x}\")\n    r = r[r.columns[r.describe().loc['std'] > 0]]\n    df = df.join(r)\n\nwith timer('number of emoji stat'):\n    r = d[text_cols].applymap(\n        lambda x: len(re.findall(u'[\\U00010000-\\U000fffff]',\n                                 x)) if type(x) == str else x).astype(\n        np.float32).rename(columns=lambda x: f\"emoji_{x}\")\n    r = r[r.columns[r.describe().loc['std'] > 0]]\n    df = df.join(r)\nft = dict(zip(df.columns.tolist(), ['q'] * df.shape[1]))\nft['price'] = 'float'\nflt_types = {**flt_types, **ft}\ndf['price'] = d['price'].astype(np.float32)\ndel d, r, ft\ngc.collect()\nX = X.join(df.loc[train.index])\nXe = Xe.join(df.loc[test.index])\n\nflt_cols = X.columns.tolist()\nflt_types = dict(zip(flt_cols, ['q'] * len(flt_cols)))\n\ndel df, train, test\ngc.collect()\nXt = np.load('Xt.npy').item(0)\nXt = hstack([X.values, Xt])\nnp.save('Xt', Xt)\ndel X, Xt\ngc.collect()\nXte = np.load('Xte.npy').item(0)\nXte = hstack([Xe.values, Xte])\nnp.save('Xte', Xte)\ndel Xe, Xte\ngc.collect()\n\n# # ***********************end floats****************************\n\n\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\n# ***********************cat encoded starts****************************\ncat_cols = ['region', 'city',\n            'parent_category_name', 'category_name',\n            'user_type', 'image_top_1',\n            'item_seq_number', 'user_id']\nnull_cols = ['price', 'description', 'param_1',\n             'param_2', 'param_3', 'image']\n\ncols = [index, *cat_cols,\n        *null_cols, 'activation_date']\nwith timer('Reading raw data > cat columns'):\n    train = pd.read_csv(f'{Path}/train.csv',\n                        parse_dates=['activation_date'],\n                        engine='c',\n                        usecols=cols,\n                        index_col=index)\n    test = pd.read_csv(f'{Path}/test.csv',\n                       parse_dates=['activation_date'],\n                       usecols=cols,\n                       engine='c',\n                       index_col=index)\n\ntrain['city'] = train['region'] + '_' + train['city']\ntest['city'] = test['region'] + '_' + test['city']\n\nwith timer('Creating isnull columns'):\n    cs = ['image_top_1', *null_cols]\n    for c in cs:\n        hc = f'null_{c}'\n        train[hc] = hc + '_' + train[c].isnull().astype(int).astype(str)\n        test[hc] = hc + '_' + test[c].isnull().astype(int).astype(str)\n        null_cols.append(hc)\ntrain.drop(null_cols, axis=1, inplace=True)\ntest.drop(null_cols, axis=1, inplace=True)\ngc.collect()\nwith timer('Creating date statistics'):\n    train['day'] = 'day_' + train[\"activation_date\"].dt.day.astype(str)\n    test['day'] = 'day_' + test[\"activation_date\"].dt.day.astype(str)\n\n    train['wday'] = 'wday_' + train[\"activation_date\"].dt.weekday.astype(str)\n    test['wday'] = 'wday_' + test[\"activation_date\"].dt.weekday.astype(str)\n\n    train['activation_date'] = 'yday_' + \\\n        train[\"activation_date\"].dt.dayofyear.astype(str)\n    test['activation_date'] = 'yday_' + \\\n        test[\"activation_date\"].dt.dayofyear.astype(str)\ndate_cols = ['day', 'wday', 'activation_date']\n\nfor c in cat_cols:\n    with timer(f'Preparing {c}'):\n        if c == 'image_top_1':\n            train[c] = f'{c}_' + train[c].fillna(FNAS).apply(\n                lambda x: str(int(x)) if type(x) != str else x)\n            test[c] = f'{c}_' + test[c].fillna(FNAS).apply(\n                lambda x: str(int(x)) if type(x) != str else x)\n        else:\n            train[c] = f'{c}_' + train[c].fillna(FNAS).astype(str)\n            test[c] = f'{c}_' + test[c].fillna(FNAS).astype(str)\n\n\ndatac = train.append(test)\nwith timer(f'Fitting all cat data with label encoder'):\n    les = LabelEncoder().fit(datac.unstack())\n    categories = dict(zip(les.classes_, range(len(les.classes_))))\n    for c in datac.columns:\n        with timer(f'transforming {c} with label encoder'):\n            datac[c] = les.transform(datac[c])\nif encode:\n    with timer(f'Fitting hotencoder'):\n        enc = OneHotEncoder(dtype=bool)\n        enc.fit(datac)\n        del datac\n        gc.collect()\n    with timer(f'Transforming train vian hotencoder'):\n        x = enc.transform(train.apply(lambda col: les.transform(col)))\n        cat_cols = train.columns.tolist()\n        cat_type = dict(zip(cat_cols, ['i'] * len(cat_cols)))\n        del train\n        gc.collect()\n        Xt = np.load('Xt.npy').item(0)\n        Xt = hstack([Xt, x]).tocsr()\n        np.save('Xt', Xt)\n        del x, Xt\n        gc.collect()\n    with timer(f'Transforming test vian hotencoder'):\n        x = enc.transform(test.apply(lambda col: les.transform(col)))\n        del test\n        gc.collect()\n        Xte = np.load('Xte.npy').item(0)\n        Xte = hstack([Xte, x]).tocsr()\n        np.save('Xte', Xte)\n        del x, Xte\n        gc.collect()\nelse:\n    with timer(f'Transforming train via label encoder'):\n        train = train.apply(lambda col: les.transform(col).astype(np.uint32))\n        cat_cols = train.columns.tolist()\n        cat_types = dict(zip(cat_cols, ['i'] * len(cat_cols)))\n        Xt = np.load('Xt.npy').item(0)\n        Xt = hstack([Xt, train.values]).tocsr()\n        np.save('Xt', Xt)\n        del train, Xt\n        gc.collect()\n    with timer(f'Transforming test via label encoder'):\n        test = test.apply(lambda col: les.transform(col).astype(np.uint32))\n        Xte = np.load('Xte.npy').item(0)\n        Xte = hstack([Xte, test.values]).tocsr()\n        np.save('Xte', Xte)\n        del test, Xte\n        gc.collect()\n# # ***********************cat ends****************************\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dtypes = {**flt_types, **cat_types, **txt_types}\ncols = [*flt_cols, *cat_cols, *txt_cols]\n\n\n# # ***********************save starts***************************\n\ndct = {'flt_cols': flt_cols,\n       'cat_cols': cat_cols,\n       'txt_cols': txt_cols,\n       'flt_types': flt_types,\n       'cat_types': cat_types,\n       'txt_types': txt_types,\n       'dtypes': dtypes,\n       'cols': cols,\n       'idf': idf,\n       'y': y}\ndel flt_cols, cat_cols, txt_cols\ndel flt_types, cat_types, txt_types\ndel dtypes, cols, idf, y\ngc.collect()\n\nwith open(f'dct_enc{int(encode)}.pkl', mode='wb') as f:\n    pickle.dump(dct, f)\n    f.close()\ndel dct\ngc.collect()\n# # ***********************save ends***************************"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# # ******************load data starts***************************\nwith open('col_type.pkl', mode='rb') as f:\n    dct = pickle.load(f)\n    f.close()\n\nXt = np.load(f'Xt.npy').item(0)\nXte = np.load(f'Xte.npy').item(0)\n# # ***********************load data  ends***************************\n\n\n# ********************* prepare data for xgb training**************\nwith timer('Preparing data for training'):\n    splt_index = list(range(len(dct['y'])))\n    np.random.shuffle(splt_index)\n    n = int(np.ceil(V_SPLT * len(dct['y'])))\n    tr_ind = splt_index[:n]\n    vl_ind = splt_index[n:]\n\n    dtrain_x = xgb.DMatrix(data=Xt[tr_ind, :],\n                           label=dct['y'].values[tr_ind],\n                           feature_names=dct['cols'],\n                           feature_types=dct['dtypes'].values())\n    dvalid_x = xgb.DMatrix(data=Xt[vl_ind, :],\n                           label=dct['y'].values[vl_ind],\n                           feature_names=dct['cols'],\n                           feature_types=dct['dtypes'].values())\n    dtest_x = xgb.DMatrix(data=Xte,\n                          feature_names=dct['cols'],\n                          feature_types=dct['dtypes'].values())\nparam_x = dict(eval_metric=\"rmse\",\n               verbose=1,\n               objective=\"reg:logistic\",\n               booster='gbtree',\n               eta=0.05,\n               max_depth=18,\n               min_child_weight=11,\n               gamma=0,\n               subsample=0.85,\n               colsample_bytree=0.7,\n               reg_alpha=2.0,\n               reg_lambda=0,\n               n_jobs=16)\n\n# ***********************prepare data for training end xgb**************\n\n# ***********************training model xgb starts************************\nwith timer(f'training xgb model'):\n    xgb_clf = xgb.train(param_x,\n                        evals=[\n                            #                         (dtrain_x,'train' ),\n                            (dvalid_x, 'valid')],\n                        dtrain=dtrain_x,\n                        num_boost_round=80000,\n                        verbose_eval=20,\n                        early_stopping_rounds=2000)\n\n# ***********************training model lgbm starts****************\n\n# ***********************plot model lgbm ****************************\nf, ax = plt.subplots(figsize=[7, 16])\nxgb.plot_importance(xgb_clf, max_num_features=100, ax=ax)\nplt.title(\"Xgboost Feature Importance\")\nplt.savefig('xgimport.png')\n# ***********************create submission with model lgbm ***********\ntest = pd.read_csv(f'{Path}sample_submission.csv')\ntest[target] = xgb_clf.predict(dtest_x).clip(0.0, 1.0)\ntest.to_csv(f\"xgb_agg_text{time.strftime('%d-%m_%H:%M')}.csv\", index=False)\n\n# ***********************training model xgb ends*********************\n# LGBM\n# ***********************prepare data lgbm for training**************\nwith timer('Preparing data for lgbm training'):\n    splt_index = list(range(len(dct['y'])))\n    np.random.shuffle(splt_index)\n    n = int(np.ceil(V_SPLT * len(dct['y'])))\n    tr_ind = splt_index[:n]\n    vl_ind = splt_index[n:]\n    lgtrain = lgb.Dataset(data=Xt[tr_ind, :],\n                          label=dct['y'].values[tr_ind],\n                          feature_name=dct['cols'],\n                          categorical_feature=dct['cat_cols'])\n    lgvalid = lgb.Dataset(data=Xt[vl_ind, :],\n                          label=dct['y'].values[vl_ind],\n                          feature_name=dct['cols'],\n                          categorical_feature=dct['cat_cols'])\n    lgtest = lgb.Dataset(data=Xte,\n                         feature_name=dct['cols'],\n                         categorical_feature=dct['cat_cols'])\n\n    lgbm_params = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'num_leaves': 32,\n        'max_depth': 18,\n        'learning_rate': 0.02,\n        'feature_fraction': 0.6,\n        'verbose': 0,\n        'num_threads': 8\n    }\n# ***********************prepare data for lgbm training end *********\n# ***********************training model lgbm starts*************\nwith timer(f'training lgbm model'):\n    lgb_clf = lgb.train(\n        lgbm_params,\n        lgtrain,\n        num_boost_round=16000,\n        valid_sets=[lgvalid],\n        valid_names=['train', 'valid'],\n        early_stopping_rounds=500,\n        verbose_eval=100\n    )\n# ***********************training model lgbm starts****************\n\n# ***********************plot model lgbm ****************************\nf, ax = plt.subplots(figsize=[7, 16])\nlgb.plot_importance(lgb_clf, max_num_features=100, ax=ax)\nplt.title(\"Light GBM Feature Importance\")\nplt.savefig('lgbimport.png')\n# **************create submission with model lgbm ***************\ntest = pd.read_csv(f'{Path}sample_submission.csv')\ntest[target] = lgb_clf.predict(lgtest).clip(0.0, 1.0)\ntest.to_csv(f\"lgb_agg_text{time.strftime('%d-%m_%H:%M')}.csv\", index=False)\n\n\n"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}